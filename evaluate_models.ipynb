{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from models.cnn_lstm import CNN_LSTM\n",
    "from models.cnn import CNN\n",
    "from models.lstm import LSTM\n",
    "from models.mlp import MLP\n",
    "from utils import load_model, DATA_DIR, BATCH_SIZE, NUM_WORKER\n",
    "from loader.fi_loader import FIDataset\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import os\n",
    "from itertools import product\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Generate Testing Statistics",
   "id": "ccdfab40af878431"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "testing_results = {\n",
    "    'model_type': [],\n",
    "    'train_data_size': [],\n",
    "    'prediction_horizon': [],\n",
    "    'accuracy': [],\n",
    "    'precision': [],\n",
    "    'recall': [],\n",
    "    'f1': []\n",
    "}"
   ],
   "id": "a6b769ee11fe56e4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "testing_params = product(zip(['CNN_LSTM', 'CNN', 'LSTM', 'MLP'], [CNN_LSTM, CNN, LSTM, MLP]), [1, 3, 5, 8], [0, 2, 4])\n",
    "\n",
    "for (model_name, model_type), cf, k in tqdm(testing_params):\n",
    "    model_path = os.path.join('.', 'trained_models', model_name)\n",
    "    trained_model_path = os.path.join(model_path, f'{model_name}_Zscore_CF{cf}_pred_{k}.pth')\n",
    "    print(trained_model_path)\n",
    "\n",
    "    trained_model = load_model(model_type, trained_model_path)\n",
    "\n",
    "    test_data = FIDataset(DATA_DIR, 'Zscore', cf, k=k, train=False)\n",
    "    test_loader = DataLoader(dataset=test_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKER)\n",
    "\n",
    "    trained_model.eval()\n",
    "    trained_model.to(trained_model.device)\n",
    "\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(trained_model.device, dtype=torch.float32), targets.to(trained_model.device, dtype=torch.int64)\n",
    "            outputs = trained_model(inputs)\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "    accuracy = accuracy_score(all_targets, all_predictions)\n",
    "    precision = precision_score(all_targets, all_predictions, average='weighted', zero_division=0)\n",
    "    recall = recall_score(all_targets, all_predictions, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(all_targets, all_predictions, average='weighted', zero_division=0)\n",
    "\n",
    "    testing_results['model_type'].append(model_name)\n",
    "    testing_results['train_data_size'].append(cf)\n",
    "    testing_results['prediction_horizon'].append(k)\n",
    "    testing_results['accuracy'].append(accuracy)\n",
    "    testing_results['precision'].append(precision)\n",
    "    testing_results['recall'].append(recall)\n",
    "    testing_results['f1'].append(f1)"
   ],
   "id": "5774effc13c0ed6c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Save Testing Stats",
   "id": "2a019335a5bbd122"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "testing_results = pd.DataFrame(testing_results)\n",
    "with open('testing_results.pkl', 'wb') as f:\n",
    "    pickle.dump(testing_results, f)"
   ],
   "id": "49b6d8935861a390",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "testing_results",
   "id": "f81d4716ac247c5a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "f5e2f62dbf7b6da5",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
