{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from itertools import product\n",
    "\n",
    "from utils import *\n",
    "from loader.fi_loader import *\n",
    "from models.cnn_lstm import CNN_LSTM\n",
    "from models.cnn import CNN\n",
    "from train import batch_train"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Train LOB learner for each of the cross validation datasets and normalization method",
   "id": "70e9eb305eacec91"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T04:34:49.575754Z",
     "start_time": "2025-04-12T04:24:25.573435Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for cf, method in product(range(1, 10), ['Zscore', 'MinMax']):\n",
    "    train_data = FIDataset(DATA_DIR, method, cf, train=True)\n",
    "    test_data = FIDataset(DATA_DIR, method, cf, train=False)\n",
    "    train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKER)\n",
    "    test_loader = DataLoader(dataset=test_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKER)\n",
    "\n",
    "    lob_model = CNN_LSTM()\n",
    "    lob_model.to(lob_model.device)\n",
    "    model_name = f'CNN_LSTM_{method}_CF{cf}'\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(lob_model.parameters(), lr=0.0001)\n",
    "\n",
    "    batch_train(model_name, lob_model, criterion, optimizer, train_loader, test_loader, EPOCHS)"
   ],
   "id": "e551855d1df8e63d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Epoch 1/30, '\n",
      "        Train Loss: 1.0254, Train Acc:  0.5047, '\n",
      "        Validation Loss: 1.0244, Validation Acc:  0.4929, '\n",
      "        Duration: 0:00:30.427857\n",
      "\n",
      "        Epoch 2/30, '\n",
      "        Train Loss: 0.9414, Train Acc:  0.5979, '\n",
      "        Validation Loss: 1.0104, Validation Acc:  0.5084, '\n",
      "        Duration: 0:00:16.186705\n",
      "\n",
      "        Epoch 3/30, '\n",
      "        Train Loss: 0.8916, Train Acc:  0.6534, '\n",
      "        Validation Loss: 1.0062, Validation Acc:  0.5220, '\n",
      "        Duration: 0:00:16.252047\n",
      "\n",
      "        Epoch 4/30, '\n",
      "        Train Loss: 0.8585, Train Acc:  0.6869, '\n",
      "        Validation Loss: 1.0077, Validation Acc:  0.5207, '\n",
      "        Duration: 0:00:16.365338\n",
      "\n",
      "        Epoch 5/30, '\n",
      "        Train Loss: 0.8356, Train Acc:  0.7100, '\n",
      "        Validation Loss: 0.9987, Validation Acc:  0.5313, '\n",
      "        Duration: 0:00:16.807592\n",
      "\n",
      "        Epoch 6/30, '\n",
      "        Train Loss: 0.8126, Train Acc:  0.7365, '\n",
      "        Validation Loss: 1.0115, Validation Acc:  0.5216, '\n",
      "        Duration: 0:00:16.452770\n",
      "\n",
      "        Epoch 7/30, '\n",
      "        Train Loss: 0.8011, Train Acc:  0.7461, '\n",
      "        Validation Loss: 0.9998, Validation Acc:  0.5343, '\n",
      "        Duration: 0:00:16.316070\n",
      "\n",
      "        Epoch 8/30, '\n",
      "        Train Loss: 0.7883, Train Acc:  0.7595, '\n",
      "        Validation Loss: 0.9972, Validation Acc:  0.5392, '\n",
      "        Duration: 0:00:15.887017\n",
      "\n",
      "        Epoch 9/30, '\n",
      "        Train Loss: 0.7764, Train Acc:  0.7720, '\n",
      "        Validation Loss: 1.0030, Validation Acc:  0.5312, '\n",
      "        Duration: 0:00:15.809042\n",
      "\n",
      "        Epoch 10/30, '\n",
      "        Train Loss: 0.7697, Train Acc:  0.7788, '\n",
      "        Validation Loss: 1.0039, Validation Acc:  0.5327, '\n",
      "        Duration: 0:00:16.106432\n",
      "\n",
      "        Epoch 11/30, '\n",
      "        Train Loss: 0.7630, Train Acc:  0.7852, '\n",
      "        Validation Loss: 1.0094, Validation Acc:  0.5275, '\n",
      "        Duration: 0:00:15.761217\n",
      "\n",
      "        Epoch 12/30, '\n",
      "        Train Loss: 0.7568, Train Acc:  0.7924, '\n",
      "        Validation Loss: 1.0065, Validation Acc:  0.5316, '\n",
      "        Duration: 0:00:16.058538\n",
      "\n",
      "        Epoch 13/30, '\n",
      "        Train Loss: 0.7506, Train Acc:  0.7982, '\n",
      "        Validation Loss: 1.0028, Validation Acc:  0.5361, '\n",
      "        Duration: 0:00:16.102977\n",
      "\n",
      "        Epoch 14/30, '\n",
      "        Train Loss: 0.7423, Train Acc:  0.8075, '\n",
      "        Validation Loss: 1.0045, Validation Acc:  0.5333, '\n",
      "        Duration: 0:00:16.095223\n",
      "\n",
      "        Epoch 15/30, '\n",
      "        Train Loss: 0.7389, Train Acc:  0.8110, '\n",
      "        Validation Loss: 1.0100, Validation Acc:  0.5286, '\n",
      "        Duration: 0:00:16.074503\n",
      "\n",
      "        Epoch 16/30, '\n",
      "        Train Loss: 0.7348, Train Acc:  0.8142, '\n",
      "        Validation Loss: 1.0093, Validation Acc:  0.5287, '\n",
      "        Duration: 0:00:16.091415\n",
      "\n",
      "        Epoch 17/30, '\n",
      "        Train Loss: 0.7280, Train Acc:  0.8212, '\n",
      "        Validation Loss: 1.0041, Validation Acc:  0.5341, '\n",
      "        Duration: 0:00:16.129455\n",
      "\n",
      "        Epoch 18/30, '\n",
      "        Train Loss: 0.7255, Train Acc:  0.8241, '\n",
      "        Validation Loss: 1.0119, Validation Acc:  0.5266, '\n",
      "        Duration: 0:00:16.211784\n",
      "\n",
      "        Epoch 19/30, '\n",
      "        Train Loss: 0.7186, Train Acc:  0.8317, '\n",
      "        Validation Loss: 1.0165, Validation Acc:  0.5220, '\n",
      "        Duration: 0:00:16.582341\n",
      "\n",
      "        Epoch 20/30, '\n",
      "        Train Loss: 0.7154, Train Acc:  0.8344, '\n",
      "        Validation Loss: 1.0125, Validation Acc:  0.5267, '\n",
      "        Duration: 0:00:15.941889\n",
      "\n",
      "        Epoch 21/30, '\n",
      "        Train Loss: 0.7154, Train Acc:  0.8347, '\n",
      "        Validation Loss: 1.0108, Validation Acc:  0.5283, '\n",
      "        Duration: 0:00:16.232903\n",
      "\n",
      "        Epoch 22/30, '\n",
      "        Train Loss: 0.7126, Train Acc:  0.8370, '\n",
      "        Validation Loss: 1.0106, Validation Acc:  0.5297, '\n",
      "        Duration: 0:00:16.073427\n",
      "\n",
      "        Epoch 23/30, '\n",
      "        Train Loss: 0.7086, Train Acc:  0.8418, '\n",
      "        Validation Loss: 1.0085, Validation Acc:  0.5321, '\n",
      "        Duration: 0:00:15.915281\n",
      "\n",
      "        Epoch 24/30, '\n",
      "        Train Loss: 0.7071, Train Acc:  0.8430, '\n",
      "        Validation Loss: 1.0165, Validation Acc:  0.5230, '\n",
      "        Duration: 0:00:16.044112\n",
      "\n",
      "        Epoch 25/30, '\n",
      "        Train Loss: 0.7052, Train Acc:  0.8448, '\n",
      "        Validation Loss: 1.0191, Validation Acc:  0.5213, '\n",
      "        Duration: 0:00:16.006577\n",
      "\n",
      "        Epoch 26/30, '\n",
      "        Train Loss: 0.7011, Train Acc:  0.8487, '\n",
      "        Validation Loss: 1.0097, Validation Acc:  0.5294, '\n",
      "        Duration: 0:00:16.311018\n",
      "\n",
      "        Epoch 27/30, '\n",
      "        Train Loss: 0.6999, Train Acc:  0.8505, '\n",
      "        Validation Loss: 1.0171, Validation Acc:  0.5226, '\n",
      "        Duration: 0:00:16.126043\n",
      "\n",
      "        Epoch 28/30, '\n",
      "        Train Loss: 0.6965, Train Acc:  0.8539, '\n",
      "        Validation Loss: 1.0179, Validation Acc:  0.5223, '\n",
      "        Duration: 0:00:17.629326\n",
      "\n",
      "        Epoch 29/30, '\n",
      "        Train Loss: 0.6948, Train Acc:  0.8548, '\n",
      "        Validation Loss: 1.0188, Validation Acc:  0.5217, '\n",
      "        Duration: 0:00:14.679207\n",
      "\n",
      "        Epoch 30/30, '\n",
      "        Train Loss: 0.6918, Train Acc:  0.8583, '\n",
      "        Validation Loss: 1.0258, Validation Acc:  0.5144, '\n",
      "        Duration: 0:00:14.781564\n",
      "model saved\n",
      "\n",
      "        Epoch 1/30, '\n",
      "        Train Loss: 1.0333, Train Acc:  0.4904, '\n",
      "        Validation Loss: 1.0402, Validation Acc:  0.4533, '\n",
      "        Duration: 0:00:15.533481\n",
      "\n",
      "        Epoch 2/30, '\n",
      "        Train Loss: 0.9585, Train Acc:  0.5781, '\n",
      "        Validation Loss: 1.0288, Validation Acc:  0.4831, '\n",
      "        Duration: 0:00:14.906037\n",
      "\n",
      "        Epoch 3/30, '\n",
      "        Train Loss: 0.9156, Train Acc:  0.6260, '\n",
      "        Validation Loss: 1.0406, Validation Acc:  0.4775, '\n",
      "        Duration: 0:00:15.072410\n",
      "\n",
      "        Epoch 4/30, '\n",
      "        Train Loss: 0.8846, Train Acc:  0.6580, '\n",
      "        Validation Loss: 1.0458, Validation Acc:  0.4810, '\n",
      "        Duration: 0:00:14.479605\n",
      "\n",
      "        Epoch 5/30, '\n",
      "        Train Loss: 0.8631, Train Acc:  0.6818, '\n",
      "        Validation Loss: 1.0254, Validation Acc:  0.5024, '\n",
      "        Duration: 0:00:15.238322\n",
      "\n",
      "        Epoch 6/30, '\n",
      "        Train Loss: 0.8466, Train Acc:  0.6984, '\n",
      "        Validation Loss: 1.0445, Validation Acc:  0.4850, '\n",
      "        Duration: 0:00:15.960912\n",
      "\n",
      "        Epoch 7/30, '\n",
      "        Train Loss: 0.8344, Train Acc:  0.7110, '\n",
      "        Validation Loss: 1.0385, Validation Acc:  0.4900, '\n",
      "        Duration: 0:00:15.141077\n",
      "\n",
      "        Epoch 8/30, '\n",
      "        Train Loss: 0.8228, Train Acc:  0.7218, '\n",
      "        Validation Loss: 1.0419, Validation Acc:  0.4891, '\n",
      "        Duration: 0:00:15.150418\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[12], line 14\u001B[0m\n\u001B[1;32m     11\u001B[0m criterion \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mCrossEntropyLoss()\n\u001B[1;32m     12\u001B[0m optimizer \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39moptim\u001B[38;5;241m.\u001B[39mAdam(lob_model\u001B[38;5;241m.\u001B[39mparameters(), lr\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.0001\u001B[39m)\n\u001B[0;32m---> 14\u001B[0m \u001B[43mbatch_train\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlob_model\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mEPOCHS\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/AFM423_FinalProject/train.py:35\u001B[0m, in \u001B[0;36mbatch_train\u001B[0;34m(model_name, model, criterion, optimizer, train_loader, val_loader, epochs)\u001B[0m\n\u001B[1;32m     33\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(outputs, targets)\n\u001B[1;32m     34\u001B[0m \u001B[38;5;66;03m# Backward and optimize\u001B[39;00m\n\u001B[0;32m---> 35\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[1;32m     36\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m     37\u001B[0m train_loss\u001B[38;5;241m.\u001B[39mappend(loss\u001B[38;5;241m.\u001B[39mitem())\n",
      "File \u001B[0;32m~/Desktop/Academic/4B/CS 480/.venv/lib/python3.13/site-packages/torch/optim/optimizer.py:493\u001B[0m, in \u001B[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    488\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    489\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    490\u001B[0m                 \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresult\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    491\u001B[0m             )\n\u001B[0;32m--> 493\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    494\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_optimizer_step_code()\n\u001B[1;32m    496\u001B[0m \u001B[38;5;66;03m# call optimizer step post hooks\u001B[39;00m\n",
      "File \u001B[0;32m~/Desktop/Academic/4B/CS 480/.venv/lib/python3.13/site-packages/torch/optim/optimizer.py:91\u001B[0m, in \u001B[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     89\u001B[0m     torch\u001B[38;5;241m.\u001B[39mset_grad_enabled(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdefaults[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdifferentiable\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[1;32m     90\u001B[0m     torch\u001B[38;5;241m.\u001B[39m_dynamo\u001B[38;5;241m.\u001B[39mgraph_break()\n\u001B[0;32m---> 91\u001B[0m     ret \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     92\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m     93\u001B[0m     torch\u001B[38;5;241m.\u001B[39m_dynamo\u001B[38;5;241m.\u001B[39mgraph_break()\n",
      "File \u001B[0;32m~/Desktop/Academic/4B/CS 480/.venv/lib/python3.13/site-packages/torch/optim/adam.py:244\u001B[0m, in \u001B[0;36mAdam.step\u001B[0;34m(self, closure)\u001B[0m\n\u001B[1;32m    232\u001B[0m     beta1, beta2 \u001B[38;5;241m=\u001B[39m group[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbetas\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m    234\u001B[0m     has_complex \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_init_group(\n\u001B[1;32m    235\u001B[0m         group,\n\u001B[1;32m    236\u001B[0m         params_with_grad,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    241\u001B[0m         state_steps,\n\u001B[1;32m    242\u001B[0m     )\n\u001B[0;32m--> 244\u001B[0m     \u001B[43madam\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    245\u001B[0m \u001B[43m        \u001B[49m\u001B[43mparams_with_grad\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    246\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgrads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    247\u001B[0m \u001B[43m        \u001B[49m\u001B[43mexp_avgs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    248\u001B[0m \u001B[43m        \u001B[49m\u001B[43mexp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    249\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmax_exp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    250\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstate_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    251\u001B[0m \u001B[43m        \u001B[49m\u001B[43mamsgrad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mamsgrad\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    252\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhas_complex\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhas_complex\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    253\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbeta1\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta1\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    254\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbeta2\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta2\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    255\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mlr\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    256\u001B[0m \u001B[43m        \u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mweight_decay\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    257\u001B[0m \u001B[43m        \u001B[49m\u001B[43meps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43meps\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    258\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmaximize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmaximize\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    259\u001B[0m \u001B[43m        \u001B[49m\u001B[43mforeach\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mforeach\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    260\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcapturable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcapturable\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    261\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdifferentiable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdifferentiable\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    262\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfused\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfused\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    263\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgrad_scale\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mgrad_scale\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    264\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfound_inf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfound_inf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    265\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    267\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m loss\n",
      "File \u001B[0;32m~/Desktop/Academic/4B/CS 480/.venv/lib/python3.13/site-packages/torch/optim/optimizer.py:154\u001B[0m, in \u001B[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    152\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m disabled_func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    153\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 154\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/Academic/4B/CS 480/.venv/lib/python3.13/site-packages/torch/optim/adam.py:876\u001B[0m, in \u001B[0;36madam\u001B[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001B[0m\n\u001B[1;32m    873\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    874\u001B[0m     func \u001B[38;5;241m=\u001B[39m _single_tensor_adam\n\u001B[0;32m--> 876\u001B[0m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    877\u001B[0m \u001B[43m    \u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    878\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    879\u001B[0m \u001B[43m    \u001B[49m\u001B[43mexp_avgs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    880\u001B[0m \u001B[43m    \u001B[49m\u001B[43mexp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    881\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmax_exp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    882\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstate_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    883\u001B[0m \u001B[43m    \u001B[49m\u001B[43mamsgrad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mamsgrad\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    884\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhas_complex\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhas_complex\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    885\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbeta1\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta1\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    886\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbeta2\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta2\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    887\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlr\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    888\u001B[0m \u001B[43m    \u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mweight_decay\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    889\u001B[0m \u001B[43m    \u001B[49m\u001B[43meps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43meps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    890\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmaximize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmaximize\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    891\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcapturable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcapturable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    892\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdifferentiable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdifferentiable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    893\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_scale\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgrad_scale\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    894\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfound_inf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfound_inf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    895\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/Academic/4B/CS 480/.venv/lib/python3.13/site-packages/torch/optim/adam.py:478\u001B[0m, in \u001B[0;36m_single_tensor_adam\u001B[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001B[0m\n\u001B[1;32m    475\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    476\u001B[0m         denom \u001B[38;5;241m=\u001B[39m (exp_avg_sq\u001B[38;5;241m.\u001B[39msqrt() \u001B[38;5;241m/\u001B[39m bias_correction2_sqrt)\u001B[38;5;241m.\u001B[39madd_(eps)\n\u001B[0;32m--> 478\u001B[0m     \u001B[43mparam\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43maddcdiv_\u001B[49m\u001B[43m(\u001B[49m\u001B[43mexp_avg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdenom\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalue\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[43mstep_size\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    480\u001B[0m \u001B[38;5;66;03m# Lastly, switch back to complex view\u001B[39;00m\n\u001B[1;32m    481\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m amsgrad \u001B[38;5;129;01mand\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mis_complex(params[i]):\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 12
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
